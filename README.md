# Roll-E: Meccanum Wheel LLM Controlled Robot
![Robot](resources/front_photo.jpg)

This project explores integrating large language models (LLMs) with mobile robotics, enabling natural language interaction, visual scene understanding, and localization. While currently implemented on a mobile robot, the approach is adaptable to various robotic platforms.

## Overview
ğŸ¤– **What if your robot had its own brain?**  
Roll-E is designed to combine LLM-based reasoning with real-world robotics, allowing for natural language control, autonomous movement, and basic task execution. An android smartphone serves as the robotâ€™s sensor hub, providing:

- **RGB Camera Data** for visual processing
- **WebXR-Based Position Tracking** for navigation
- **Microphone and Speakers** for voice interaction

ğŸ”¥ **Key Features**
- âœ… **Natural Language Understanding** â€“ Processes and responds to voice commands
- âœ… **Omnidirectional Movement** â€“ Smooth motion control with ramped velocity 
- âœ… **Smartphone-Based Sensing** â€“ Utilizes a phone for vision, localization, and audio processing
- âœ… **ROS2 Integration** â€“ Modular, scalable framework for future enhancements

## System Architecture

### High Level Overview
The following diagram illustrates the core components and their interactions:

![System Architecture](resources/Phone_sensor_graph.png)

### Hardware
- **Robot Platform**: 4 Wheel Meccanum Drive Robot
- **Microcontroller**: Raspberry Pi 4B
- **Motor Driver**: 2 x L298N
- **Motors**: Hobby DC Motors with gearbox
- **Battery unit** - 12V AA battery Pack
- **Power Bank** - Powers the Raspberry Pi

## Robot In Action

[Demo video](https://youtu.be/FYqDDZrb-XY)


## Ongoing Work
I am actively developing and refining several aspects of the project:
- ğŸ”§ **ROS2 Integration:** Converting JavaScript-based WebXR localization into a ROS 2 node for better data handling
- ğŸ™ **Enhanced NLP Processing:** Refining the LLM for improved contextual understanding
- ğŸš€ **Autonomous Navigation:** Implementing Nav2 for autonomous path planning
- ğŸ“± **User Interface Improvements:** Developing an intuitive monitoring and control interface

## Future Improvements
Planned enhancements for the near future include:
- ğŸ¯ **Object Recognition & Manipulation:** Adding a robotic arm to perform basic tasks (similar to LeRobot)
- ğŸ”— **Expanded Voice Commands:** Enabling API calls for app control and task automation
- ğŸŒ **Remote Monitoring & Control:** Developing a dedicated app for real-time remote oversight 

---

Feel free to reach out with questions and ideas. Let's push the boundaries of robotics together!ğŸ¤–
